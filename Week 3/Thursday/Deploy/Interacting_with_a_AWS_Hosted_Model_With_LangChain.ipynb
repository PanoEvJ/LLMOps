{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PanoEvJ/LLMOps/blob/main/Week%203/Thursday/Deploy/Interacting_with_a_AWS_Hosted_Model_With_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yLZ0jE2yXV7i"
      },
      "outputs": [],
      "source": [
        "model_api_gateway = \"https://e6g370y6pe.execute-api.us-east-1.amazonaws.com/default/Llama2API\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : [\n",
        "       [{\"role\" : \"system\", \"content\" : \"You are an expert in Python.\"},\n",
        "       {\"role\" : \"user\", \"content\" : \"How would I write a function to determine the Nth Fibonacci number?\"}]\n",
        "    ],\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : 256,\n",
        "        \"top_p\" : 0.9,\n",
        "        \"temperature\" : 0.1\n",
        "    }\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIkG6RakXhb8",
        "outputId": "3eac1d7e-e85b-4f9c-a0c3-f0f2517e496b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generation': {'role': 'assistant', 'content': \" Great! I'd be happy to help you write a function to determine the Nth Fibonacci number in Python.\\n\\nThe Fibonacci sequence is a series of numbers where each number is the sum of the two preceding numbers, starting from 0 and 1. The sequence begins like this: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, and so on.\\n\\nTo write a function to determine the Nth Fibonacci number in Python, you can use the following approach:\\n\\n1. Define a function called `fibonacci` that takes an integer `n` as input.\\n2. Use a recursive approach to calculate the `n`th Fibonacci number. The basic idea is to call the function with `n-1` and `n-2` as arguments, and then use the formula `fibonacci(n) = fibonacci(n-1) + fibonacci(n-2)`.\\n3. Return the calculated value of `fibonacci(n)`.\\n\"}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain"
      ],
      "metadata": {
        "id": "bWy12euzgf-r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class Llama2SageMaker(LLM):\n",
        "    max_new_tokens: int = 256\n",
        "    top_p: float = 0.9\n",
        "    temperature: float = 0.1\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Llama2SageMaker\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "    ) -> str:\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "\n",
        "        json_body = {\n",
        "            \"inputs\" : [\n",
        "              [{\"role\" : \"user\", \"content\" : prompt}]\n",
        "            ],\n",
        "            \"parameters\" : {\n",
        "                \"max_new_tokens\" : self.max_new_tokens,\n",
        "                \"top_p\" : self.top_p,\n",
        "                \"temperature\" : self.temperature\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = requests.post(model_api_gateway, json=json_body)\n",
        "\n",
        "        return response.json()[0][\"generation\"][\"content\"]\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\n",
        "            \"max_new_tokens\" : self.max_new_tokens,\n",
        "            \"top_p\" : self.top_p,\n",
        "            \"temperature\" : self.temperature\n",
        "        }"
      ],
      "metadata": {
        "id": "K0dSlPfngh4o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Llama2SageMaker()"
      ],
      "metadata": {
        "id": "EygPRQj9jG-D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"How much wood could a woodchuck chuck if a wood chuck could chuck wood?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "5FRbcRKgjI76",
        "outputId": "2947837a-33fc-4861-b246-e19a5f360406"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The answer to this tongue twister is a bit of a trick question! Woodchucks, also known as groundhogs, are not actually able to chuck wood. Woodchucks are burrowing animals that live in the ground and are known for their ability to dig tunnels and burrows. They do not have the ability to move or manipulate wood.\\n\\nSo, if a woodchuck could chuck wood, it would be able to move a very small amount of wood, perhaps a few small branches or twigs at a time. However, it's important to remember that woodchucks are not capable of chucking wood, so this is purely a hypothetical scenario!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U pymupdf"
      ],
      "metadata": {
        "id": "6AjBDFCYlwmA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"https://arxiv.org/pdf/2005.11401v4.pdf\")"
      ],
      "metadata": {
        "id": "IyrFqDRnmWEZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U openai chromadb tiktoken"
      ],
      "metadata": {
        "id": "UOdpBdXxn7qO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "openai_api_key = getpass.getpass(\"Please provide your OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgMOeJSwnr4D",
        "outputId": "7b4b585e-29d0-4001-cb92-42437afc8320"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ],
      "metadata": {
        "id": "rkAvOadmnjDQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm, retriever=index.vectorstore.as_retriever(), return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "7opJDJLxoEUa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Retrieval Augmented Generation?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "2qXK-jOCpQ23",
        "outputId": "cb439cc0-9677-4c06-a55b-7fb708b6dab1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Based on the provided context, Retrieval Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based models and generation models to improve the performance of knowledge-intensive NLP tasks. RAG models use a retrieval component to access and manipulate knowledge in a non-parametric manner, and a generation component to generate text that is factual and specific. The goal of RAG is to unify previous successes in incorporating retrieval into individual tasks and show that a single retrieval-based architecture can achieve strong performance across several tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Retrieval Augmented Generation?\"\n",
        "result = qa_chain({\"query\": question})\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhrUwVDguy5Q",
        "outputId": "f20d2678-330a-429e-9ab4-35472feefcb6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is Retrieval Augmented Generation?',\n",
              " 'result': ' Based on the provided context, Retrieval Augmented Generation (RAG) is a technique that combines the strengths of retrieval-based models and generation models to improve the performance of knowledge-intensive NLP tasks. RAG models use a combination of parametric and non-parametric memory to generate high-quality text, and they have shown promising results in open-domain question answering tasks. The key idea behind RAG is to leverage the strengths of retrieval-based models, which are good at retrieving relevant information, and generation models, which are good at generating coherent and fluent text. By combining these two approaches, RAG models can generate more informative and accurate text than either approach alone.',\n",
              " 'source_documents': [Document(page_content='including less of emphasis on lightly editing a retrieved item, but on aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, and\\ncould represent promising future work.\\n6\\nDiscussion\\nIn this work, we presented hybrid generation models with access to parametric and non-parametric\\nmemory. We showed that our RAG models obtain state of the art results on open-domain QA. We\\nfound that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual\\nand speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating\\nits effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model\\nwithout requiring any retraining. In future work, it may be fruitful to investigate if the two components', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/tmp/tmp36x5nbzx/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 8, 'producer': 'pdfTeX-1.40.21', 'source': '/tmp/tmp36x5nbzx/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
              "  Document(page_content='generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our\\nwork uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single\\nretrieval-based architecture is capable of achieving strong performance across several tasks.\\n8', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/tmp/tmp36x5nbzx/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 7, 'producer': 'pdfTeX-1.40.21', 'source': '/tmp/tmp36x5nbzx/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
              "  Document(page_content='[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:\\nRetrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https:\\n//arxiv.org/abs/2002.08909.\\n[21] Tatsunori B Hashimoto,\\nKelvin Guu,\\nYonatan Oren,\\nand Percy S Liang.\\nA\\nretrieve-and-edit\\nframework\\nfor\\npredicting\\nstructured\\noutputs.\\nIn\\nS.\\nBengio,\\nH. Wallach,\\nH. Larochelle,\\nK. Grauman,\\nN. Cesa-Bianchi,\\nand R. Garnett,\\ned-\\nitors,\\nAdvances\\nin\\nNeural\\nInformation\\nProcessing\\nSystems\\n31,\\npages\\n10052–\\n10062.\\nCurran\\nAssociates,\\nInc.,\\n2018.\\nURL\\nhttp://papers.nips.cc/paper/\\n8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.\\npdf.\\n[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-\\nedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for\\nComputational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/tmp/tmp36x5nbzx/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 11, 'producer': 'pdfTeX-1.40.21', 'source': '/tmp/tmp36x5nbzx/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}),\n",
              "  Document(page_content='Retrieval-Augmented Generation for\\nKnowledge-Intensive NLP Tasks\\nPatrick Lewis†‡, Ethan Perez⋆,\\nAleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\\nMike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\\n†Facebook AI Research; ‡University College London; ⋆New York University;\\nplewis@fb.com\\nAbstract\\nLarge pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-\\nstream NLP tasks. However, their ability to access and precisely manipulate knowl-\\nedge is still limited, and hence on knowledge-intensive tasks, their performance\\nlags behind task-speciﬁc architectures. Additionally, providing provenance for their\\ndecisions and updating their world knowledge remain open research problems. Pre-\\ntrained models with a differentiable access mechanism to explicit non-parametric', metadata={'author': '', 'creationDate': 'D:20210413004838Z', 'creator': 'LaTeX with hyperref', 'file_path': '/tmp/tmp36x5nbzx/tmp.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20210413004838Z', 'page': 0, 'producer': 'pdfTeX-1.40.21', 'source': '/tmp/tmp36x5nbzx/tmp.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''})]}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VSimopJBKXxJ"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}